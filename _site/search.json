[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "farganaislamblog",
    "section": "",
    "text": "Funny Town\n\n\n\n\n\n\n\nfun project\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nFargana Islam\n\n\n\n\n\n\n  \n\n\n\n\nA Geospatial Analysis of Rental Price of Apartments in Dhaka\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nFargana Islam\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nFargana Islam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/RentalListingDhaka.html",
    "href": "posts/post-with-code/RentalListingDhaka.html",
    "title": "A Geospatial Analysis of Rental Price of Apartments in Dhaka",
    "section": "",
    "text": "This is probably what the title of this article could have been if it was a clickbait attempt. But I think the image below serves a similar purpose. Now I will walk you through what steps I undertook to get this comparative visualization of rental apartment costs around Dhaka.\n\nLibraries That We Need\n\nlibrary(sf)\nlibrary(raster)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(terra)\nlibrary(OpenStreetMap)\nlibrary(osmdata)\nlibrary(leaflet)\n\nLoading the shape files of three different layers of administrative distribution. I downloaded all this from data.world. There are 4 different administrative area categories in Bangladesh. ADMIN1: the divisions, ADMIN2: the districts, ADMIN3: upazillas/sectors (the latter for cities), ADMIN4: unions/wards (the former for rural areas, the latter for cities). Since I am only interested in the city of Dhaka which is a part of the district Dhaka, I am only interested in the layers ADMIN3 and ADMIN4. I will also call areas from these layers as ADMIN3 area and ADMIN4 area respectively.\n\nshape_file_3 = st_read(\"bgd_admbnda_adm3_bbs_20201113.shp\")\n\nReading layer `bgd_admbnda_adm3_bbs_20201113' from data source \n  `/Users/farganaislam/farganaislamblog/posts/post-with-code/bgd_admbnda_adm3_bbs_20201113.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 544 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 88.00863 ymin: 20.59061 xmax: 92.68031 ymax: 26.63451\nGeodetic CRS:  WGS 84\n\n\nOnly keeping the ADMIN3 areas that are situated in the Dhaka metropolitan area.\n\nshape_file_3 = shape_file_3 %>% subset(ADM2_EN == 'Dhaka')\nshape_file_3 = shape_file_3 %>% filter(Shape_Area < 1.00e-02)\nADM3_list <- as.vector(st_drop_geometry(shape_file_3['ADM3_EN']))[['ADM3_EN']]\n\nLoad ADMIN4 areas and keep only those that are inside the same area as ADM3_list\n\nshape_file_4 = st_read(\"shape_file_4_small.shp\")\n\nReading layer `shape_file_4_small' from data source \n  `/Users/farganaislam/farganaislamblog/posts/post-with-code/shape_file_4_small.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 203 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 90.00457 ymin: 23.52215 xmax: 90.51279 ymax: 24.04435\nGeodetic CRS:  WGS 84\n\nshape_file_4 = shape_file_4 %>% subset(ADM2_EN == 'Dhaka') \nshape_file_4p = shape_file_4 %>% subset(ADM3_EN %in% ADM3_list)\n\nVisualizing the map on leaflet\n\nbbox_val <- st_bbox(shape_file_4p)\n\n(m <- leaflet() %>%\n  addTiles() %>% \n  addProviderTiles(\"OpenStreetMap.HOT\", group = \"Humanitarian\") %>% \n  addTiles(options = providerTileOptions(noWrap = TRUE), group = \"Default\") %>%\n  # addMarkers(lng = dhaka_long, lat = dhaka_lat, popup='Dhaka') %>% \n  # addRectangles(bbox_val[[1]], bbox_val[[2]], bbox_val[[3]], bbox_val[[4]]) %>%\n  addPolygons(data = shape_file_3, fillOpacity = 0.1, label = ~ADM3_EN, weight=1.5,\n              labelOptions = labelOptions(noHide = TRUE, textOnly = TRUE, style = list(\"font-size\"=\"8px\", \"color\"=\"blue\"))\n              ))\n\n\n\n\n\nNow let’s read the rental price data. This is from a kaggle dataset. The provider scraped it from the popular website bproperty.com and released it under CC0 license.\n\nproperty_data <- read.csv('property_listing_data_in_Bangladesh.csv')\nsummary(property_data)\n\n    title               beds               bath               area          \n Length:8294        Length:8294        Length:8294        Length:8294       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n    adress              type             purpose            flooPlan        \n Length:8294        Length:8294        Length:8294        Length:8294       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n     url            lastUpdated           price          \n Length:8294        Length:8294        Length:8294       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\nLet’s fix the incorrect spelling of the column names.\n\nproperty_data <- property_data %>% rename(address = adress) %>% rename(floorPlan = flooPlan)\n\nInitially we are only concerned with the following columns: ‘adress’, ‘area’, ‘beds’, ‘bath’, ‘price’. Let’s look at a few rows to get a good idea. Later, I will use the trick from the link https://www.cararthompson.com/posts/2022-09-09-automating-sentences-with-r/ to make the above list more dynamic\n\nhead(property_data[c('address', 'area', 'beds', 'bath', 'price')])\n\n                                     address       area beds bath       price\n1    Priyanka City, Sector 12, Uttara, Dhaka 1,500 sqft    3    3 30 Thousand\n2 Modhubag, Boro Maghbazar, Maghbazar, Dhaka               4    3 22 Thousand\n3            Block G, Bashundhara R-A, Dhaka 1,850 sqft    3    3 35 Thousand\n4            Block G, Bashundhara R-A, Dhaka 1,850 sqft    3    3 35 Thousand\n5                    Sector 5, Uttara, Dhaka 1,500 sqft    3    3 30 Thousand\n6                   Block J, Banasree, Dhaka 1,500 sqft    3    3 20 Thousand\n\n\nLet’s count how many of the entries are actually in Dhaka.\n\ntotal_num_of_entries <- count(property_data)\n(num_of_entries_in_dhaka <- grepl(\"Dhaka\", property_data$address) %>% sum())\n\n[1] 6092\n\n\nSo, about 73.5 % of the entries are from Dhaka. That is good for us. Let’s filter the other entries from property_data.\n\nproperty_data <- property_data %>% subset(grepl(\"Dhaka\", property_data$address))\n\nOne thing to note is that property_data does not have any spatial information. The address is a string that one is tempted to look up in a web mapping platform. In fact, I am just going to do that using Google Maps API. However, I cannot disclose my API in the open web. So as a compromise, I will write a function called pseudo_mutate_geocode that will essentially simulate the behavior of ggmap::mutate_geocode. In order to do that, I called the actual mutate_geocode function and queried Google Maps with the addresses listed in property_data. I stored the query output in a new dataframe and saved it to a csv file. The pseudo_mutate_geocode function actually reads from that csv file to simulate the same behavior. However, if you want to query Google Maps using your own API key, that option is given to you as well. All you have to do is set the api_key variable’s value to your API key.\n\npseudo_mutate_geocode <- function(data, location) {\n  left_join(data, read.csv('cached_google_map_query.csv')[c(\"address\", \"lon\", \"lat\")], by = location)\n}\n\napi_key <- \"NO_API_KEY_PROVIDED\"\nif (api_key == \"NO_API_KEY_PROVIDED\") {\n  head(property_data <- property_data %>% pseudo_mutate_geocode(\"address\"))[c(\"address\", \"lon\", \"lat\")]\n} else {\n  library(ggmap)\n  register_google(api_key)\n  head(property_data <- property_data %>% mutate_geocode(address))[c(\"address\", \"lon\", \"lat\")]\n}\n\n                                     address      lon      lat\n1    Priyanka City, Sector 12, Uttara, Dhaka 90.37978 23.86853\n2 Modhubag, Boro Maghbazar, Maghbazar, Dhaka 90.40898 23.74936\n3            Block G, Bashundhara R-A, Dhaka 90.43314 23.82441\n4            Block G, Bashundhara R-A, Dhaka 90.43314 23.82441\n5                    Sector 5, Uttara, Dhaka 90.39073 23.86467\n6                   Block J, Banasree, Dhaka 90.43314 23.76194\n\n\n\nhead(property_data[c('address', 'lon', 'lat', 'area', 'beds', 'bath', 'price')])\n\n                                     address      lon      lat       area beds\n1    Priyanka City, Sector 12, Uttara, Dhaka 90.37978 23.86853 1,500 sqft    3\n2 Modhubag, Boro Maghbazar, Maghbazar, Dhaka 90.40898 23.74936               4\n3            Block G, Bashundhara R-A, Dhaka 90.43314 23.82441 1,850 sqft    3\n4            Block G, Bashundhara R-A, Dhaka 90.43314 23.82441 1,850 sqft    3\n5                    Sector 5, Uttara, Dhaka 90.39073 23.86467 1,500 sqft    3\n6                   Block J, Banasree, Dhaka 90.43314 23.76194 1,500 sqft    3\n  bath       price\n1    3 30 Thousand\n2    3 22 Thousand\n3    3 35 Thousand\n4    3 35 Thousand\n5    3 30 Thousand\n6    3 20 Thousand\n\n\nNow I use the lon and lat attribute as location to convert the dataframe into an sf. I also add ADM4_PCODE for each row from the shape_file_4p sf.\n\nproperty_data_sf <- st_as_sf(property_data, coords=c(\"lon\", \"lat\"), crs = 4326)\n(number_of_rows <- count(property_data_sf)[[1]])\n\n[1] 6092\n\n\nAfter the conversion, we have 6092 left. That means the google map query could not find location value for the other 0 rows.\nNow let’s try to see which ADMIN4 area each of this location point is situated in.\n\nproperty_data_sf <- st_join(property_data_sf, shape_file_4p['ADM4_PCODE'], st_intersects, left=TRUE)\n\nLet’s count the number of rows left na values we got in ADM4_PCODE when st_join could not find any admin area in which the rental property is located.\n\nsum(is.na(property_data_sf$ADM4_PCODE))\n\n[1] 0\n\n\nThat is really good. This means we don’t have to filter any of the rows for now.\nNow let’s look at the other non-spatial attributes in property_data. Previously, we have seen all the columns are actually character strings even though some of them could benefit from being converted to numeric. There are also categorical attributes such as type and purpose. Let’s look at what are the unique possible values of these attributes and their frequencies. The columns beds and bath are not exactly categorical but they are integers and looking at their possible values could also be helpful.\n\nlapply(property_data_sf[c('type', 'purpose', 'beds', 'bath')] %>% st_drop_geometry(), table)\n\n$type\n\nApartment  Building    Duplex \n     6028        19        45 \n\n$purpose\n\nFor Rent \n    6092 \n\n$beds\n\n1 Bed    14    17    18     2    20    21    22     3    32     4     5     6 \n  117     1     1     1  1957     3     2     1  3496     1   452    46     9 \n    7 \n    5 \n\n$bath\n\n1 Bath     10      2      3      4      5      6      7      8 \n   327     10   2355   2405    873    103     12      1      6 \n\n\nLooking at the type attribute we can see that almost all the entries are apartments. Even though we have some building and duplex rows, they are likely to skew the rent distribution. That’s why I filter entries that are not apartments.\n\nproperty_data_sf <- property_data_sf %>% filter(type == 'Apartment')\n\nAll the apartments are for rent so we don’t have to do anything with the purpose attribute.\n‘1 Bed’ is the only value that cannot be easily converted to numeric. Same goes for ‘1 Bath’. Let’s delete the non-numeric portion in those cases using gsub. After that we convert them to numeric.\n\nproperty_data_sf$beds <- gsub(pattern = '1 Bed', replacement= '1', property_data_sf$beds) %>% as.numeric()\nproperty_data_sf$bath <- gsub(pattern = '1 Bath', replacement= '1', property_data_sf$bath) %>% as.numeric()\n\narea and price are in character string as well and we need to convert them to numeric. We need to first identify the possible word suffixes in these attributes. Let’s use regex [0-9] to delete the numeric portion and see all the unique suffixes.\n\n(area_suffixes <- gsub(\"[0-9]\", \"\", property_data_sf$area) %>% table())\n\n.\n         sqft , sqft \n  1973    468   3587 \n\n\nWe have 1973 rows with no suffixes and other rows contain sqft as the suffix. We also have to get rid of the ‘,’ between the digits.\n\nproperty_data_sf$area <- as.numeric(property_data_sf$area %>% gsub(' sqft', '', .) %>% gsub(',', '', .))\n\nNow let’s find out the suffixes in price.\n\n(price_suffixes <- gsub(\"[0-9]\", \"\", property_data_sf$price) %>% table())\n\n.\n      Lakh   Thousand     . Lakh . Thousand \n        93       5424        223        288 \n\n\nThis time we have two suffixes and depending on which of them is present, we need to multiply the numeric portion with 100000 or 1000. Let’s use a combination of grepl and gsub to achieve this.\n\nproperty_data_sf$rent <- property_data_sf$price %>%\n                         gsub(\" Thousand\", \"\", .) %>%\n                         gsub(\" Lakh\", \"\", .) %>%\n                          as.numeric() *\n                         (1000*(property_data_sf$price %>% grepl(\" Thousand\", .)) \n                           +100000*(property_data_sf$price %>% grepl(\" Lakh\", .)))\n\nI rename this as rent since it suits the context better.\nLet’s look at the summary of the 4 numerical attributes.\n\nsummary(property_data_sf[c('beds', 'bath', 'area', 'rent')] %>% st_drop_geometry())\n\n      beds            bath            area            rent       \n Min.   :1.000   Min.   :1.000   Min.   :  600   Min.   :  7000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 1150   1st Qu.: 15000  \n Median :3.000   Median :3.000   Median : 1300   Median : 20000  \n Mean   :2.719   Mean   :2.672   Mean   : 1404   Mean   : 31917  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.: 1500   3rd Qu.: 30000  \n Max.   :6.000   Max.   :6.000   Max.   :12000   Max.   :924000  \n                                 NA's   :1973                    \n\n\nWe have some na values in area. I don’t want to remove these rows yet, as there could be a possibility that the attributes are higly correlated and in that case we can interpolate area value for these rows.\nLet’s look at the correlation matrix of the 4 attributes.\n\ncor(property_data_sf[c('beds', 'bath', 'area', 'rent')] %>% filter(!is.na(area)) %>%  st_drop_geometry())\n\n           beds       bath       area       rent\nbeds 1.00000000 0.78838448 0.09969126 0.45605851\nbath 0.78838448 1.00000000 0.09151683 0.46801114\narea 0.09969126 0.09151683 1.00000000 0.09237041\nrent 0.45605851 0.46801114 0.09237041 1.00000000\n\n\nInterestingly, we got a high correlation between the beds, bath, and rent attribute but area is loosely correlated with these 3. Normally, area and rent are presumed to be highly correlated. However, in a dense metropolitan city, rent depends on numerous other factors. But if that were the case, the correlation would not have been so high between rent, beds, and bath. The area attribute might require some sanitization.\nLet’s first look at a boxplot of area ~ beds to get some idea. I choose beds as the x axis because it has only 6 possible values and a more important descriptor of a rental property than the number of baths.\n\nboxplot(area ~ beds, property_data_sf, horizontal=TRUE)\n\n\n\n\narea seems to have a longer positive tail than the negative tail in each subgroup. 12000 sqft is unusually large for a 2/3 bedroom apartment. (To put that into perspective, a basketball court is 4700 sqft). Keeping in mind that these entries are originally created by the property owners from a third world country, it might be the case that some of the values were incorrectly put down. Let’s look at additional attributes of a single row to see whether we can cross-check.\n\ncat(paste(sprintf(\"%s = %s\", \n                  names(property_data[1,]), \n                  as.character(property_data[1,])), \n          collapse = \"\\n\"))\n\ntitle = To Get A Trouble Free Life You Can Take Rent This 1600 Sq Ft House In Uttara 12\nbeds = 3\nbath = 3\narea = 1,500 sqft\naddress = Priyanka City, Sector 12, Uttara, Dhaka\ntype = Apartment\npurpose = For Rent\nfloorPlan = https://images-cdn.bproperty.com/thumbnails/1543504-240x180.webp\nurl = https://www.bproperty.com/en/property/details-5454098.html\nlastUpdated = July 29, 2022\nprice = 30 Thousand\nlon = 90.3797847\nlat = 23.8685318\n\n\nFirst thing to notice here is that the area information is embedded in the title. The second thing is that the two information do not match even though the difference is small. Let’s quickly look at how many entries actually have the “sq” or “ft” pattern in them.\n\n(area_embedded_in_title <- property_data_sf$title %>% tolower() %>% grepl(\" sq| ft\", .) %>% summary())\n\n   Mode   FALSE    TRUE \nlogical     976    5052 \n\n\nAbout 83.8088918 % of the rows have the area value embedded in the title. That is even more than the number of rows that have a valid value in the area attribute (4055 ). Let’s try to extract that information.\nLet’s look at a few title first that fits the pattern.\n\n(title_vector <- property_data_sf[(property_data_sf$title %>% tolower() %>% grepl(\" sq| ft\", .)),]$title) %>% head(10)\n\n [1] \"To Get A Trouble Free Life You Can Take Rent This 1600 Sq Ft House In Uttara 12\"                                                        \n [2] \"Visit This 1400 Square Feet Apartment Which Is Vacant For Rent In Maghbazar\"                                                            \n [3] \"1500 Sq Ft Apartment Is Up For Rent In Bashundhara, Near Atimkhana Madrasa.\"                                                            \n [4] \"Check This Beautiful 1250 Sq Ft Flat For Rent At Banasree\"                                                                              \n [5] \"At Hatirpool 1180 Square Feet Flat For Rent Close To Dhanmondi Ideal College\"                                                           \n [6] \"Check This Apartment For Rent Which Is Covering An Area Of 3000 Sq Ft In Baridhara\"                                                     \n [7] \"See, this 1807 SQ Ft beautiful apartment and make it your new home which is up for rent in Mohammadpur nearby Baitus Salam Jame Masjid.\"\n [8] \"Properly-constructed 1950 SQ FT flat is now offering to you in Bashundhara R-A for rent\"                                                \n [9] \"800 Sq Ft Apartment For Rent In Dakshin Khan\"                                                                                           \n[10] \"Offering You 2200 Sq Ft Amazing Apartment For Rent In Mirpur DOHS\"                                                                      \n\n\nThe area size precedes the word containing sq/ft in all the 10 instances above. If we tokenize a title by splitting the string by whitespace and find the index of the token containing sq/ft, then one less than that index is what we are looking for. Let’s define a function that will do this for us.\n\nextract_area <- function(string) {\n  tokens <- unlist(strsplit(tolower(string), \" \"))\n  indices <- which(grepl(\"^sq|^ft\", tokens[2:length(tokens)]))\n  # return(length(indices))\n  index <- indices[1] + 1\n  # return(index)\n  # return(tokens[index-1])\n  area_size <- gsub(\"[^[:digit:]]\", \"\", tokens[index-1])\n  area_size <- gsub(',', '', area_size)\n  return(as.numeric(area_size))\n}\n\nBefore applying it on property_data_sf, let’s verify it’s effectiveness in title_vector. I apply extract_area on title_vector and summarize the result.\n\n#extracted_areas <- purrr::map(title_vector, purrr::possibly(.f = extract_area, otherwise = NA))\nextracted_areas <- unlist(lapply(title_vector, extract_area))\nsummary(extracted_areas)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    300     800    1200    1314    1550    6600       1 \n\n\nFrom the summary, it looks like we have only 1 NA value which means we could not extract area information from that row. Let’s see the title of that row.\n\ntitle_vector[extracted_areas %>% is.na()]\n\n[1] \"Ready Flat Is Now For Rent In Mohammadpur Nearby Mohammadpur Tokyo Square\"\n\n\n“Tokyo Square” is a residential area in Dhaka City and this threw off the extract_area function which was unable to convert “Tokyo” to numeric. This is an acceptable failure.\nNow, let’s modify the function extract_area to make it work in cases where there is no sq/ft in the title and after that we will directly apply it on the title attribute of property_data_sf.\n\nextract_area <- function(string) {\n  tokens <- unlist(strsplit(tolower(string), \" \"))\n  indices <- which(grepl(\"^sq|^ft\", tokens[2:length(tokens)]))\n  if (length(indices)<1) {\n    return(NA)\n  }\n  # return(length(indices))\n  index <- indices[1] + 1\n  # return(index)\n  # return(tokens[index-1])\n  area_size <- gsub(\"[^[:digit:]]\", \"\", tokens[index-1])\n  area_size <- gsub(',', '', area_size)\n  return(as.numeric(area_size))\n}\n\n\nproperty_data_sf$area_extracted <- unlist(lapply(property_data_sf$title, extract_area))\nsummary(property_data_sf$area_extracted)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    300     800    1200    1314    1550    6600     977 \n\n\nLet’s see the correlation of area_extracted with other 3 attributes.\n\ncor(property_data_sf[c('beds', 'bath', 'area_extracted', 'rent')] %>% filter(!is.na(area_extracted)) %>%  st_drop_geometry())\n\n                    beds      bath area_extracted      rent\nbeds           1.0000000 0.7858722      0.7425423 0.4588450\nbath           0.7858722 1.0000000      0.7694966 0.4665285\narea_extracted 0.7425423 0.7694966      1.0000000 0.8172428\nrent           0.4588450 0.4665285      0.8172428 1.0000000\n\n\nAwesome! The new area is highly correlated with beds, bath, and rent. Let’s look at the boxplot of area_extracted ~ beds to get an idea of what changed.\n\nboxplot(area_extracted ~ beds, property_data_sf, horizontal=TRUE)\n\n\n\n\nThe positive tail in each group has shrunk and the values make realistic sense. The following is my hypothesis on why area_extracted turned out to be more realistic than the original column area. Title is a character string and it represents a summary of the property at a glance. That’s why most property owners were careful in writing it down. The area information was a numeric field that received less attention and as a result generated unreliable values. We will proceed with area_extract so let’s rename the variables.\n\nproperty_data_sf <- property_data_sf %>% rename(area_from_data = area) %>% rename(area = area_extracted)\n\n5051 is enough records for us to work with for now. On a later update, I will try to do geospatial interpolation to find area of the missing values.\nThe usual approach to apartment rental cost analysis is to compare rent per square feet of the data entries. Let’s mutate a rent_per_sqft column.\n\nproperty_data_sf$rent_per_sqft <- property_data_sf$rent / property_data_sf$area\n\nWe want to do a comparison among the ADMIN4 areas in term of rent_per_sqft. Let’s group the data by ADM4_PCODE that we added before and calculate mean of the rent_per_sqft for each ADMIN4 area.\n\nmean_rent_per_sqft <- property_data_sf %>%\n  st_drop_geometry() %>%\n  group_by(ADM4_PCODE) %>%\n  summarise(rent_per_sqft = mean(rent_per_sqft, na.rm=TRUE))\nsummary(mean_rent_per_sqft$rent_per_sqft)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  11.54   16.67   20.01   21.19   23.30   51.95       2 \n\n\nThere are 2 NA values which means for 2 areas we had no entries with a valid rent_per_sqft value. Let’s drop those 2 areas from mean_rent_per_sqft.\n\nmean_rent_per_sqft <- mean_rent_per_sqft %>% filter(!is.na(rent_per_sqft))\n\n[NOTE: I want to add a boxplot/raincloud of rent_per_sqft grouped by ADM3_PCODE later]\nWe want this information to actually be in the shape_file_4p sf. Let’s copy it there and visualize.\n\nshape_file_4p$rent_per_sqft <- mean_rent_per_sqft$rent_per_sqft[match(shape_file_4p$ADM4_PCODE, mean_rent_per_sqft$ADM4_PCODE)]\nplot((shape_file_4p %>% filter(!is.na(rent_per_sqft)))['rent_per_sqft'])\n\n\n\n\nFew things to note here.\nThe rent_per_sq_ft is homogeneous across most regions except for a few ADMIN4 area in the middle. Those belong to the corporate portion of the city. But it is hard to get a sense of the level of the disparity among the areas from the color value.\nI expected some of the areas at the periphery to not have enough information which turned out to be true. Looking at the south portion of the city this effect becomes obvious.\nSome middle areas also seem to have no rental property listed. One of them situates the Airport, another is the industrial area.\nNow I want to see the rent_per_sqft in a 3D setting to properly assess the disparity.\nFirst, I will load the stars library and rasterize the spatial shape_file_4p.\n\nshape_file_4p <- shape_file_4p %>%\n  subset(!is.na(rent_per_sqft))\nshape_file_4p$rent_per_sqft <- shape_file_4p$rent_per_sqft * 30\n\n\nlibrary(stars)\ngrd = st_as_stars(st_bbox(shape_file_4p %>% dplyr::select(rent_per_sqft, geometry)), nx=2500, ny=2500)\nraster_map <- st_rasterize(shape_file_4p %>% dplyr::select(rent_per_sqft, geometry), grd)\n\nLet’s use the cool rayshader library to plot this raster map.\n\nlibrary(rayshader)\n\nr <- rast(raster_map)\nelmat = raster_to_matrix(r)\n\nAlso, I want to overlay a streetmap picture of Dhaka over the rayshader 3D map for the inhabitants to quickly have an idea which portion is actually which area. This is a non-standard step but it ended up looking good so I am keeping it like this. Due to space constraint, I am not adding the steps of actually downloading the picture from ArcGIS but I followed the steps from this brilliant guide Tutorial: Adding Open Street Map Data to Rayshader Maps in R – Rayverse Blog (tylermw.com).\n\noverlay_file <- \"images_dhaka.png\"\noverlay_img <- png::readPNG(overlay_file)\n\nLet’s finally draw the 3D map.\n\nelmat %>%\n  sphere_shade(texture = \"imhof2\") %>%\n  # add_water(detect_water(elmat), color = \"desert\") %>%\n  add_overlay(overlay_img, alphalayer = 1.0) %>%\n  # add_overlay(overlay_img_2, alphalayer = 1.0) %>%\n  # add_overlay(osm_data) %>%\n  add_shadow(ray_shade(elmat, zscale = 3), 0.5) %>%\n  add_shadow(ambient_shade(elmat), 0) %>%\n  plot_3d(elmat, zscale = 2, fov = 0, theta = 45, zoom = 0.75, phi = 45, windowsize = c(1600, 1600))\n\n\nrender_snapshot('~/Desktop/dhaka_3d.jpg')"
  },
  {
    "objectID": "posts/funny-us-town-names/index.html",
    "href": "posts/funny-us-town-names/index.html",
    "title": "Funny Town",
    "section": "",
    "text": "Recently, I came across an web article titled 65 Funny Town Names Across the U.S. Ever since I started living in the states, I was awed by the simultaneous banality and eccentricity of the country’s town names. As I was scrolling through the ad explosion of the article, I kept wondering what one has to do make such a list and also whether the list covers all the funny town names after all. So I decided to do this fun endeavor. Here are a few pictures showcasing some of the interesting names I found. The images are courtesy of Google Maps.\n\n\n\nA church located in Man, West Virginia\n\n\n\n\n\nA motor shop located in Moody, Alabama\n\n\n\n\n\nA water tank located in Magazine, Arkansas\n\n\nThe endeavor would be incomplete if I don’t list the names from the article first. But the invasive advertisement made it hard for me to scroll any farther down and I decided to scrape from the web article using RSelenium.\n\nlibrary(rvest)\nlibrary(RSelenium)\nlibrary(dplyr)\n\n\nlink <- \"https://www.farandwide.com/s/wacky-town-names-47d4c1c59a624a70\"\n\nI run the following command from the terminal to start the Docker container which will act like a server docker run -d -p 4445:4444 selenium/standalone-firefox. Then I use RSelenium to launch a headless browser from the server which will read the article for me. The browser loads the webpage and keeps pressing End key to scroll down and get a glimpse of everything. Finally, it collects the names from the element of interest.\n\nremDr <- remoteDriver(\n\n  remoteServerAddr = \"localhost\",\n\n  port = 4445L,\n\n  browserName = \"firefox\"\n\n)\n\n\nremDr$open()\n\n\nremDr$navigate(link)\n\nThis is where the browser keeps scrolling down.\n\nwebElem <- remDr$findElement(\"xpath\", \"/html/body\")\n\n# Scroll down 10 times\nfor(i in 1:10){      \n  webElem$sendKeysToElement(list(key = \"end\"))\n  # please make sure to sleep a couple of seconds because it takes time to load contents\n  Sys.sleep(5)    \n}\n\n\npage_source <- remDr$getPageSource()\n\nI used inspect on my real browser to discover the fact that list-item-title is the element/node I am looking for. That is primarily because I am a noob. I am sure any decent-skilled web scraper would have found a smart way to do it from the headless browser.\n\nall_funny_names <- read_html(page_source[[1]]) %>% html_nodes(\".list-item-title\") %>% html_text()\n\n\n\n\nLooks like we got 2 titles. There may have been similar elements in the web page that we don’t want. Since, the article says 65 town names let’s peek at the values from 61-70 and decide where to cut-off.\n\n(all_funny_names$x %>% head(70) %>% tail(10))\n\n [1] \"Why, Arizona\"           \"Whynot, North Carolina\" \"Yum Yum, Tennessee\"    \n [4] \"Zigzag, Oregon\"         \"Zzyzx, California\"      \"Branson, Missouri\"     \n [7] \"Where to Stay\"          \"Baraboo, Wisconsin\"     \"Where to Stay\"         \n[10] \"Brainerd, Minnesota\"   \n\n\nLooks like it ends with Zzyzx, California or at index 65. Others are promotional towns featured in the article. Let’s filter everything after the 65th one.\n\nall_funny_names <- all_funny_names[1:65,]\n\nHere’s the final list.\n\nall_funny_names$x\n\n [1] \"Accident, Maryland\"                \"Aladdin, Wyoming\"                 \n [3] \"Bacon Level, Alabama\"              \"Bat Cave, North Carolina\"         \n [5] \"Bigfoot, Texas\"                    \"Bitter End, Tennessee\"            \n [7] \"Booger Hole, West Virginia\"        \"Boring, Oregon\"                   \n [9] \"Breeding, Kentucky\"                \"Bugtussle, Kentucky\"              \n[11] \"Burnout, Alabama\"                  \"Carefree, Arizona\"                \n[13] \"Center of the World, Ohio\"         \"Chicken, Alaska\"                  \n[15] \"Coke County, Texas\"                \"Cookietown, Oklahoma\"             \n[17] \"Correct, Indiana\"                  \"Cut and Shoot, Texas\"             \n[19] \"Ding Dong, Texas\"                  \"DISH, Texas\"                      \n[21] \"Early, Iowa\"                       \"Earth, Texas\"                     \n[23] \"Fifty-Six, Arkansas\"               \"Frankenstein, Missouri\"           \n[25] \"French Lick, Indiana\"              \"Funk, Nebraska\"                   \n[27] \"Good Grief, Idaho\"                 \"Greasy Corner, Arkansas\"          \n[29] \"Half Hell, North Carolina\"         \"Hell, Michigan\"                   \n[31] \"Hot Coffee, Mississippi\"           \"Hurt, Virginia\"                   \n[33] \"Hygiene, Colorado\"                 \"Intercourse, Pennsylvania\"        \n[35] \"Ketchuptown, South Carolina\"       \"Knockemstiff, Ohio\"               \n[37] \"Monkey’s Eyebrow, Kentucky\"        \"Neutral, Kansas\"                  \n[39] \"No Name, Colorado\"                 \"Normal, Illinois\"                 \n[41] \"Nothing, Arizona\"                  \"Peculiar, Missouri\"               \n[43] \"Pee Pee, Ohio\"                     \"Pfafftown, North Carolina\"        \n[45] \"Plastic, Colorado\"                 \"Poverty, Kentucky\"                \n[47] \"Random Lake, Wisconsin\"            \"Romance, Arkansas\"                \n[49] \"Rough and Ready, California\"       \"Santa Claus, Indiana\"             \n[51] \"Satan's Kingdom, Massachusetts\"    \"Slaughterville, Oklahoma\"         \n[53] \"Scratch Ankle, Alabama\"            \"Success, Missouri\"                \n[55] \"Tombstone, Arizona\"                \"Top-of-the-World, Arizona\"        \n[57] \"Truth or Consequences, New Mexico\" \"Turkey Scratch, Arkansas\"         \n[59] \"Two Egg, Florida\"                  \"Wealthy, Texas\"                   \n[61] \"Why, Arizona\"                      \"Whynot, North Carolina\"           \n[63] \"Yum Yum, Tennessee\"                \"Zigzag, Oregon\"                   \n[65] \"Zzyzx, California\"                \n\n\nThis web article is certainly not the first one to march this direction. A quick Google search gave me a book titled “The Cafeteria Lady Eats Her Way Across America: And Lives to Tell About It!” which has a chapter”A Town By Any Other Name” dedicated to stringing these names together into a narrative. Luckily, these pages are available for preview in the Google books website so if anybody wants they can check it out. There’s also a line in one of the Wrens’ song “Why into Whynot” but I have no proof they meant the towns Why, Arizona and Whynot, North Carolina.\nInspired by these, I wanted to make a comprehensive list of all the names. So I went looking for the largest list of US town names available in the internet. There were quite a few and one from the Github had 60k+ records. Of course, I am not going to comb through this list so I started thinking of better approaches.\nThe first idea that I had was to use a dictionary. Most of the words in the web article and the book were actually dictionary words except for a few goofy ones. So I decided to use the hunspell library to filter out town names that are not dictionary words.\n\nall_town_df <- read.csv('us_cities_states_counties.csv', sep=\"|\")\n\n\nlibrary(hunspell)\n\n\nselected_indices <- all_town_df$City.alias %>% hunspell_check(dict=dictionary(\"en_US\"))\nall_town_df_filtered <- all_town_df %>% filter(selected_indices)\n\nThat didn’t help much. We still have 63210 entries left. But most of them should be duplicates and a lot of them could have a really high frequency (think Springfield). We are not interested in the high frequency ones, good things come in small portion. Let’s use table() then hist() to see the frequency distribution.\n\nfrequency_table <- table(all_town_df_filtered$City.alias)\nhist(frequency_table)\n\n\n\n\nWe still have a lot of town names that are not frequent but still a dictionary word. Let’s say any town name that appears more than 4 times is more or less accepted by the people as a common town name and filter those out.\n\nless_common_town_names <- names(frequency_table[frequency_table <= 4])\nall_town_df_filtered <- all_town_df_filtered %>% filter(City.alias %in% less_common_town_names)\n\nThere’s still 3547 town names left for us to comb through. Now I want to see some of the names to come up with an idea on how to proceed.\n\nless_common_town_names %>% head(50)\n\n [1] \"88\"         \"Aaron\"      \"Abbot\"      \"Abel\"       \"Abernathy\" \n [6] \"Abilene\"    \"Abraham\"    \"Abram\"      \"Abrams\"     \"Academia\"  \n[11] \"Academy\"    \"Accident\"   \"Accord\"     \"Ace\"        \"Achilles\"  \n[16] \"Acorn\"      \"Acosta\"     \"Action\"     \"Adamant\"    \"Adelaide\"  \n[21] \"Aden\"       \"Adirondack\" \"Adkins\"     \"Admire\"     \"Adolph\"    \n[26] \"Advance\"    \"Advent\"     \"Aeneas\"     \"Africa\"     \"Agana\"     \n[31] \"Agar\"       \"Agate\"      \"Agency\"     \"Agenda\"     \"Ages\"      \n[36] \"Agnes\"      \"Agnew\"      \"Agra\"       \"Agricola\"   \"Aguilar\"   \n[41] \"Aguirre\"    \"Aiken\"      \"Ajax\"       \"Akin\"       \"Alabama\"   \n[46] \"Alabaster\"  \"Aladdin\"    \"Alamogordo\" \"Alaska\"     \"Albee\"     \n\n\nAs much as it is interesting to hear about Alabama, New York, I am looking for something that gives a stronger kick upon hearing it. There are enough Paris, Texas references in the world for it to have become trivial by now. There are also people names such as Abraham, Agnes, Adolph that are in the hunspell dictionary that are better off being filtered. In fact, therein lies the problem. hunspell dictionary is too much inclusive for our task. We need a dictionary that is less interested in the proper nouns.\nAfter looking around for a while, I found the freeDictionaryAPI which can be queried using the url https://api.dictionaryapi.dev/api/v2/entries/en/{word}. So I initially tried to use url_exists() function of RCurl library but that ended up in 429 errors (too many requests). I could have used Sys.sleep() but I decided to poke around the freeDictionaryAPI looking for ways to query with multiple words. I found something even better. There is a word list directly available in their Github page.\n\ndictionary_file <- file('freeDictionaryAPI.txt')\nfreeDictionaryAPI_words <- readLines(dictionary_file)\nclose(dictionary_file)\n\nNow let’s run the function over less_common_town_names and filter based on the result. I am using purrr::map_lgl() to avoid writing a loop in this case.\n\nwords_with_definition <- tolower(less_common_town_names) %in% freeDictionaryAPI_words\n\n\ntown_names <- less_common_town_names %>% subset(words_with_definition)\n\n\n\n\n\nset.seed(42)\nsample(town_names, 50)\n\n [1] \"Maple\"        \"Lemons\"       \"Moe\"          \"Dyke\"         \"Wrens\"       \n [6] \"Mack\"         \"Nursery\"      \"Windfall\"     \"Cc\"           \"Root\"        \n[11] \"Bighorn\"      \"Tiffany\"      \"Headland\"     \"Donna\"        \"Nineteen\"    \n[16] \"Hoard\"        \"Irons\"        \"Visa\"         \"Druid\"        \"Quay\"        \n[21] \"Gumbo\"        \"Brothers\"     \"Odds\"         \"Corn\"         \"Northwest\"   \n[26] \"Zap\"          \"Cadillac\"     \"Nook\"         \"Gravity\"      \"Ina\"         \n[31] \"Outing\"       \"Quarry\"       \"Fishers\"      \"Hon\"          \"Duke\"        \n[36] \"Bruin\"        \"Battleground\" \"Panorama\"     \"Junction\"     \"Chaparral\"   \n[41] \"Jester\"       \"Thorn\"        \"Electron\"     \"Crocus\"       \"Altitude\"    \n[46] \"Wisdom\"       \"Bandy\"        \"Zinc\"         \"Op\"           \"Seminary\"    \n\n\nWe still have 2116 names left. I printed 50 random samples above to see if this is what we were looking for from the first place. The results don’t look too bad, you definitely won’t expect most of them to be a town name. Any attempt at shortening the current list has a high probability of removing some interesting town names. That’s why I will keep it as it is for now.\nSome names that really surprised me:\n\nTea, South Dakota\nBachelor, Missouri\nBonus, Illinois\nElectron, Washington\nHome, Washington\nSweet Home, Oregon\nJunior, West Virginia\nLeisure, Indiana\nMan, West Virginia\nMagazine, Arkansas\nOil, Indiana\nPrinter, Kentucky\nRepublican, Arkansas\n\nOne final thing I want to do is show the remaining towns on the OpenStreetMap using Leaflet. But before I do that, I need to actually get their location.\n\nfunny_town_df <- all_town_df %>% filter(City.alias %in% town_names)\n\n\nfunny_town_df[\"address\"] <- paste0(funny_town_df$City.alias, \", \", funny_town_df$State.full)\n\n\napi_key <- \"API_KEY\"\nlibrary(ggmap)\nregister_google(api_key)\n\n\nfunny_town_df <- funny_town_df %>% mutate_geocode(address)\n\n\n\n\nAfter the lon and lat values are retrieved, we can quickly convert it to sf.\n\nlibrary(sf)\nlibrary(leaflet)\nlibrary(OpenStreetMap)\nlibrary(osmdata)\n\n\nfunny_town_sf <- st_as_sf(funny_town_df, coords=c(\"lon\", \"lat\"), crs = 4326)\n\nGoogle map might have looked for places with similar name elsewhere in the world when one was not found within the United States. We need to filter those out.\n\nlibrary(spData)\ndata('world')\nusa_map <- world[world$name_long == \"United States\",]\n\n\nfunny_town_sf <- funny_town_sf %>% filter(lengths(st_intersects(funny_town_sf, usa_map)) > 0)\n\n\n(m <- leaflet() %>%\n  addTiles() %>% \n  addProviderTiles(\"OpenStreetMap.HOT\", group = \"Humanitarian\") %>% \n  addTiles(options = providerTileOptions(noWrap = TRUE), group = \"Default\") %>% \n  # addMarkers(lng = dhaka_long, lat = dhaka_lat, popup='Dhaka') %>% \n  # addRectangles(bbox_val[[1]], bbox_val[[2]], bbox_val[[3]], bbox_val[[4]]) %>%\n  addMarkers(data=funny_town_sf, clusterOptions = markerClusterOptions()))"
  }
]